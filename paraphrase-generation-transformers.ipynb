{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84740,"sourceType":"datasetVersion","datasetId":46601}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. About this notebook\n\nThis notebook uses a Kaggle dataset containing Wikipedia phrases to create paraphrases and non-paraphrases that will later be used to tran a paraphrase-detection network based on transformers. THe Wikipedia sentences dataset can be found [here](https://www.kaggle.com/datasets/mikeortman/wikipedia-sentences)","metadata":{}},{"cell_type":"code","source":"import os\nfrom random import shuffle\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:20:03.324214Z","iopub.execute_input":"2024-12-21T13:20:03.324420Z","iopub.status.idle":"2024-12-21T13:20:10.509010Z","shell.execute_reply.started":"2024-12-21T13:20:03.324400Z","shell.execute_reply":"2024-12-21T13:20:10.508132Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Import Wikipedia sentences dataset from Kaggle\nsentences_path = \"/kaggle/input/wikipedia-sentences/wikisent2.txt\"\nsentences=[]\nwith open(sentences_path) as file:\n    for item in file:\n        sentences.append(item[:-1])\n        \nshuffle(sentences)\n\nlen(sentences)\n\n# Save sentences used to generate paraphrases\nsentences_for_paraphrase = sentences[:len(sentences)//2]\nwith open('/kaggle/working/sentences_for_paraphrase.txt', 'w') as f:\n    for sentence in sentences_for_paraphrase:\n        f.write(f\"{sentence}\\n\")\n        \n# Save sentences used to generate non-paraphrases\nsentences_for_non_paraphrase = sentences[len(sentences)//2:]\nwith open('/kaggle/working/sentences_for_non_paraphrase.txt', 'w') as f:\n    for sentence in sentences_for_non_paraphrase:\n        f.write(f\"{sentence}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-12-20T08:42:38.346006Z","iopub.execute_input":"2024-12-20T08:42:38.346290Z","iopub.status.idle":"2024-12-20T08:43:00.182984Z","shell.execute_reply.started":"2024-12-20T08:42:38.346269Z","shell.execute_reply":"2024-12-20T08:43:00.182271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the model \ntokenizer_paraphrase_generation = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \nmodel_paraphrase_generation = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_paraphrase_generation.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:20:10.509873Z","iopub.execute_input":"2024-12-21T13:20:10.510501Z","iopub.status.idle":"2024-12-21T13:20:33.973761Z","shell.execute_reply.started":"2024-12-21T13:20:10.510331Z","shell.execute_reply":"2024-12-21T13:20:33.973013Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53f6971e0cf49be8be25c665667294e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8fda5d535b4bef8ab28b21ff1f87e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bc9ad9694e4db4b3540ac40f456047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5c9b5dc5b7c43b898662837a72837c2"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1611754c2f3c45f2be1c230103fa3661"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Generating paraphrases\nsentences_paraphrased = []\n\nfor sentence in range(len(sentences_for_paraphrase[:100000])):\n    text =  \"paraphrase: \" + sentences_for_paraphrase[sentence] + \" </s>\"\n\n    encoding = tokenizer_paraphrase_generation.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\n    outputs = model_paraphrase_generation.generate(\n        input_ids=input_ids, attention_mask=attention_masks,\n        max_length=40,\n        do_sample=True,\n        top_k=200,\n        top_p=0.95,\n        early_stopping=True,\n        num_return_sequences=1,\n        temperature=1.3\n    )\n\n    sentences_paraphrased.append(tokenizer_paraphrase_generation.decode(outputs[0], skip_special_tokens=True,clean_up_tokenization_spaces=True))\n    \n    if (sentence % 10000) == 0:\n        print(sentence)\n        with open('/kaggle/working/sentences_paraphrased.txt', 'w') as f:\n            for sentence in sentences_paraphrased:\n                f.write(f\"{sentence}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-12-20T08:44:58.643261Z","iopub.execute_input":"2024-12-20T08:44:58.643793Z","iopub.status.idle":"2024-12-20T08:45:33.228459Z","shell.execute_reply.started":"2024-12-20T08:44:58.643762Z","shell.execute_reply":"2024-12-20T08:45:33.227219Z"}}},{"cell_type":"code","source":"sentences_path = \"/kaggle/input/sentences-non-paraphrase/sentences_for_non_paraphrase.txt\"\nsentences_non_paraphrased = []\n\nwith open(sentences_path) as file:\n    for item in file:\n        sentences_non_paraphrased.append(item[:-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:22:50.056351Z","iopub.execute_input":"2024-12-21T13:22:50.056708Z","iopub.status.idle":"2024-12-21T13:22:56.709920Z","shell.execute_reply.started":"2024-12-21T13:22:50.056679Z","shell.execute_reply":"2024-12-21T13:22:56.709214Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Generating non-paraphrases\n#sentences_non_paraphrased = []\n\nfor sentence in range(len(sentences_for_non_paraphrase[:100000])):\n    text =  \"paraphrase: \" + sentences_for_non_paraphrase[sentence] + \" </s>\"\n\n    encoding = tokenizer_paraphrase_generation.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\n    outputs = model_paraphrase_generation.generate(\n        input_ids=input_ids, attention_mask=attention_masks,\n        max_length=40,\n        do_sample=True,\n        top_k=200,\n        top_p=0.95,\n        early_stopping=True,\n        num_return_sequences=1,\n        temperature=2.0\n    )\n\n    sentences_non_paraphrased.append(tokenizer_paraphrase_generation.decode(outputs[0], skip_special_tokens=True,clean_up_tokenization_spaces=True))\n    \n    if (sentence % 10000) == 0:\n        print(sentence)\n        with open('/kaggle/working/sentences_non_paraphrased.txt', 'w') as f:\n            for sentence in sentences_non_paraphrased:\n                f.write(f\"{sentence}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}